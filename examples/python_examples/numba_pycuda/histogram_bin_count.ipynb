{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NvvmError",
     "evalue": "Failed to compile\n\nIR version 1.6 incompatible with current version 2.0\n<unnamed>: error: incompatible IR detected. Possible mix of compiler/IR from different releases.\nNVVM_ERROR_IR_VERSION_MISMATCH",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNvvmError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/rjia/file_exchange_port/notes/examples/python_examples/numba_pycuda/histogram_bin_count.ipynb Cell 1\u001b[0m line \u001b[0;36m<cell line: 47>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/rjia/file_exchange_port/notes/examples/python_examples/numba_pycuda/histogram_bin_count.ipynb#W0sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m image \u001b[39m=\u001b[39m Image\u001b[39m.\u001b[39mopen(\u001b[39m\"\u001b[39m\u001b[39m/home/rjia/Pictures/activate_call_button.png\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/rjia/file_exchange_port/notes/examples/python_examples/numba_pycuda/histogram_bin_count.ipynb#W0sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m \u001b[39m# image.show()\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/rjia/file_exchange_port/notes/examples/python_examples/numba_pycuda/histogram_bin_count.ipynb#W0sZmlsZQ%3D%3D?line=46'>47</a>\u001b[0m compute_histogram(image)\n",
      "\u001b[1;32m/home/rjia/file_exchange_port/notes/examples/python_examples/numba_pycuda/histogram_bin_count.ipynb Cell 1\u001b[0m line \u001b[0;36mcompute_histogram\u001b[0;34m(image)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/rjia/file_exchange_port/notes/examples/python_examples/numba_pycuda/histogram_bin_count.ipynb#W0sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m blockspergrid \u001b[39m=\u001b[39m (blockspergrid_x, blockspergrid_y)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/rjia/file_exchange_port/notes/examples/python_examples/numba_pycuda/histogram_bin_count.ipynb#W0sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m \u001b[39m# Launch the kernel\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/rjia/file_exchange_port/notes/examples/python_examples/numba_pycuda/histogram_bin_count.ipynb#W0sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m histogram_kernel[blockspergrid, threadsperblock](d_image, d_histogram)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/numba/cuda/dispatcher.py:491\u001b[0m, in \u001b[0;36m_LaunchConfiguration.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    490\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs):\n\u001b[0;32m--> 491\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdispatcher\u001b[39m.\u001b[39;49mcall(args, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgriddim, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mblockdim,\n\u001b[1;32m    492\u001b[0m                                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstream, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msharedmem)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/numba/cuda/dispatcher.py:625\u001b[0m, in \u001b[0;36mCUDADispatcher.call\u001b[0;34m(self, args, griddim, blockdim, stream, sharedmem)\u001b[0m\n\u001b[1;32m    623\u001b[0m     kernel \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39m(\u001b[39miter\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moverloads\u001b[39m.\u001b[39mvalues()))\n\u001b[1;32m    624\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 625\u001b[0m     kernel \u001b[39m=\u001b[39m _dispatcher\u001b[39m.\u001b[39;49mDispatcher\u001b[39m.\u001b[39;49m_cuda_call(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs)\n\u001b[1;32m    627\u001b[0m kernel\u001b[39m.\u001b[39mlaunch(args, griddim, blockdim, stream, sharedmem)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/numba/cuda/dispatcher.py:633\u001b[0m, in \u001b[0;36mCUDADispatcher._compile_for_args\u001b[0;34m(self, *args, **kws)\u001b[0m\n\u001b[1;32m    631\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m kws\n\u001b[1;32m    632\u001b[0m argtypes \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtypeof_pyval(a) \u001b[39mfor\u001b[39;00m a \u001b[39min\u001b[39;00m args]\n\u001b[0;32m--> 633\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompile(\u001b[39mtuple\u001b[39;49m(argtypes))\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/numba/cuda/dispatcher.py:794\u001b[0m, in \u001b[0;36mCUDADispatcher.compile\u001b[0;34m(self, sig)\u001b[0m\n\u001b[1;32m    791\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_can_compile:\n\u001b[1;32m    792\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mCompilation disabled\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 794\u001b[0m kernel \u001b[39m=\u001b[39m _Kernel(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpy_func, argtypes, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtargetoptions)\n\u001b[1;32m    795\u001b[0m \u001b[39m# We call bind to force codegen, so that there is a cubin to cache\u001b[39;00m\n\u001b[1;32m    796\u001b[0m kernel\u001b[39m.\u001b[39mbind()\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/numba/core/compiler_lock.py:35\u001b[0m, in \u001b[0;36m_CompilerLock.__call__.<locals>._acquire_compile_lock\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m     33\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_acquire_compile_lock\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     34\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m---> 35\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/numba/cuda/dispatcher.py:94\u001b[0m, in \u001b[0;36m_Kernel.__init__\u001b[0;34m(self, py_func, argtypes, link, debug, lineinfo, inline, fastmath, extensions, max_registers, opt, device)\u001b[0m\n\u001b[1;32m     91\u001b[0m     link \u001b[39m=\u001b[39m []\n\u001b[1;32m     93\u001b[0m \u001b[39m# A kernel needs cooperative launch if grid_sync is being used.\u001b[39;00m\n\u001b[0;32m---> 94\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcooperative \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mcudaCGGetIntrinsicHandle\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m lib\u001b[39m.\u001b[39;49mget_asm_str()\n\u001b[1;32m     95\u001b[0m \u001b[39m# We need to link against cudadevrt if grid sync is being used.\u001b[39;00m\n\u001b[1;32m     96\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcooperative:\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/numba/cuda/codegen.py:102\u001b[0m, in \u001b[0;36mCUDACodeLibrary.get_asm_str\u001b[0;34m(self, cc)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_asm_str\u001b[39m(\u001b[39mself\u001b[39m, cc\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m--> 102\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_join_ptxes(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_ptxes(cc\u001b[39m=\u001b[39;49mcc))\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/numba/cuda/codegen.py:140\u001b[0m, in \u001b[0;36mCUDACodeLibrary._get_ptxes\u001b[0;34m(self, cc)\u001b[0m\n\u001b[1;32m    136\u001b[0m     ptxes \u001b[39m=\u001b[39m [nvvm\u001b[39m.\u001b[39mllvm_to_ptx(ir, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39moptions) \u001b[39mfor\u001b[39;00m ir \u001b[39min\u001b[39;00m irs]\n\u001b[1;32m    137\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    138\u001b[0m     \u001b[39m# Otherwise, we compile all modules with NVVM at once because this\u001b[39;00m\n\u001b[1;32m    139\u001b[0m     \u001b[39m# results in better optimization than separate compilation.\u001b[39;00m\n\u001b[0;32m--> 140\u001b[0m     ptxes \u001b[39m=\u001b[39m [nvvm\u001b[39m.\u001b[39;49mllvm_to_ptx(irs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49moptions)]\n\u001b[1;32m    142\u001b[0m \u001b[39m# Sometimes the result from NVVM contains trailing whitespace and\u001b[39;00m\n\u001b[1;32m    143\u001b[0m \u001b[39m# nulls, which we strip so that the assembly dump looks a little\u001b[39;00m\n\u001b[1;32m    144\u001b[0m \u001b[39m# tidier.\u001b[39;00m\n\u001b[1;32m    145\u001b[0m ptxes \u001b[39m=\u001b[39m [x\u001b[39m.\u001b[39mdecode()\u001b[39m.\u001b[39mstrip(\u001b[39m'\u001b[39m\u001b[39m\\x00\u001b[39;00m\u001b[39m'\u001b[39m)\u001b[39m.\u001b[39mstrip() \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m ptxes]\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/numba/cuda/cudadrv/nvvm.py:688\u001b[0m, in \u001b[0;36mllvm_to_ptx\u001b[0;34m(llvmir, **opts)\u001b[0m\n\u001b[1;32m    685\u001b[0m     cu\u001b[39m.\u001b[39madd_module(mod\u001b[39m.\u001b[39mencode(\u001b[39m'\u001b[39m\u001b[39mutf8\u001b[39m\u001b[39m'\u001b[39m))\n\u001b[1;32m    686\u001b[0m cu\u001b[39m.\u001b[39mlazy_add_module(libdevice\u001b[39m.\u001b[39mget())\n\u001b[0;32m--> 688\u001b[0m \u001b[39mreturn\u001b[39;00m cu\u001b[39m.\u001b[39;49mcompile(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mopts)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/numba/cuda/cudadrv/nvvm.py:285\u001b[0m, in \u001b[0;36mCompilationUnit.compile\u001b[0;34m(self, **options)\u001b[0m\n\u001b[1;32m    282\u001b[0m c_opts \u001b[39m=\u001b[39m (c_char_p \u001b[39m*\u001b[39m \u001b[39mlen\u001b[39m(opts))(\u001b[39m*\u001b[39m[c_char_p(x\u001b[39m.\u001b[39mencode(\u001b[39m'\u001b[39m\u001b[39mutf8\u001b[39m\u001b[39m'\u001b[39m))\n\u001b[1;32m    283\u001b[0m                                   \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m opts])\n\u001b[1;32m    284\u001b[0m err \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdriver\u001b[39m.\u001b[39mnvvmCompileProgram(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_handle, \u001b[39mlen\u001b[39m(opts), c_opts)\n\u001b[0;32m--> 285\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_try_error(err, \u001b[39m'\u001b[39;49m\u001b[39mFailed to compile\u001b[39;49m\u001b[39m\\n\u001b[39;49;00m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m    287\u001b[0m \u001b[39m# get result\u001b[39;00m\n\u001b[1;32m    288\u001b[0m reslen \u001b[39m=\u001b[39m c_size_t()\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/numba/cuda/cudadrv/nvvm.py:303\u001b[0m, in \u001b[0;36mCompilationUnit._try_error\u001b[0;34m(self, err, msg)\u001b[0m\n\u001b[1;32m    302\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_try_error\u001b[39m(\u001b[39mself\u001b[39m, err, msg):\n\u001b[0;32m--> 303\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdriver\u001b[39m.\u001b[39;49mcheck_error(err, \u001b[39m\"\u001b[39;49m\u001b[39m%s\u001b[39;49;00m\u001b[39m\\n\u001b[39;49;00m\u001b[39m%s\u001b[39;49;00m\u001b[39m\"\u001b[39;49m \u001b[39m%\u001b[39;49m (msg, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_log()))\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/numba/cuda/cudadrv/nvvm.py:180\u001b[0m, in \u001b[0;36mNVVM.check_error\u001b[0;34m(self, error, msg, exit)\u001b[0m\n\u001b[1;32m    178\u001b[0m     sys\u001b[39m.\u001b[39mexit(\u001b[39m1\u001b[39m)\n\u001b[1;32m    179\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 180\u001b[0m     \u001b[39mraise\u001b[39;00m exc\n",
      "\u001b[0;31mNvvmError\u001b[0m: Failed to compile\n\nIR version 1.6 incompatible with current version 2.0\n<unnamed>: error: incompatible IR detected. Possible mix of compiler/IR from different releases.\nNVVM_ERROR_IR_VERSION_MISMATCH"
     ]
    }
   ],
   "source": [
    "from numba import cuda\n",
    "import numpy as np\n",
    "\n",
    "@cuda.jit\n",
    "def histogram_kernel(image, histogram):\n",
    "    \"\"\"\n",
    "    CUDA kernel for calculating the histogram of an image.\n",
    "    Each thread calculates the histogram for one pixel and atomically increments the histogram bin.\n",
    "    \"\"\"\n",
    "    # Calculate the position of the current thread in the grid\n",
    "    x, y = cuda.grid(2)\n",
    "    # if x < image.shape[0] and y < image.shape[1]:\n",
    "    #     # Get the pixel value\n",
    "    #     pixel_value = image[x, y]\n",
    "    #     # Atomically increment the corresponding histogram bin\n",
    "    #     cuda.atomic.add(histogram, pixel_value, 1)\n",
    "\n",
    "def compute_histogram(image):\n",
    "    \"\"\"\n",
    "    Compute the histogram of an image using CUDA.\n",
    "    \"\"\"\n",
    "    # Assuming image pixel values range from 0 to 255\n",
    "    histogram = np.zeros(256, dtype=np.uint32)\n",
    "    image = np.asarray(image)\n",
    "    \n",
    "    # Allocate memory on the device\n",
    "    d_image = cuda.to_device(image)\n",
    "    d_histogram = cuda.to_device(histogram)\n",
    "    \n",
    "    # Configure the blocks\n",
    "    threadsperblock = (16, 16)\n",
    "    blockspergrid_x = int(np.ceil(image.shape[0] / threadsperblock[0]))\n",
    "    blockspergrid_y = int(np.ceil(image.shape[1] / threadsperblock[1]))\n",
    "    blockspergrid = (blockspergrid_x, blockspergrid_y)\n",
    "    \n",
    "    # Launch the kernel\n",
    "    histogram_kernel[blockspergrid, threadsperblock](d_image, d_histogram)\n",
    "    \n",
    "    # # Copy the histogram back to the host\n",
    "    # histogram = d_histogram.copy_to_host()\n",
    "    \n",
    "    # return histogram\n",
    "\n",
    "from PIL import Image\n",
    "image = Image.open(\"/home/rjia/Pictures/activate_call_button.png\")\n",
    "# image.show()\n",
    "compute_histogram(image)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
